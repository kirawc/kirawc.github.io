<!DOCTYPE html>
<html>

  <head>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
    <link rel="stylesheet" href="index.css">
    <script src="index.js"></script>

  </head>

  <body>

      <div class = "block1">
        <div class="name">Kira Wegner-Clemens</div>
        <div class="subhead">cognitive neuroscientist</div>

      <div class = "linksbox">
        <a href="#" id="homebutton" class="button">home</a>
        <a href="#" id="projbutton" class="button">projects</a>
        <a href="#" id="pubbutton" class="button">publications</a>
      </div>

      </div>

      <div class = "block2">
        <div class="flex-container">

          <div class='image'>
          <img src = "pic.jpg", width="150"></img>

          <br><br>
          kira at gwu dot edu
          <br><br>
          <a href="CV.pdf">CV</a>
          <br><br>
          <a href="https://scholar.google.com/citations?user=tAiYT2IAAAAJ&hl=en&oi=ao">google scholar</a> /
          <a href="https://github.com/kirawc">github</a>
          <br>
          <a href="https://www.linkedin.com/in/kira-wegner-clemens/">linkedin</a> /
          <a href="https://www.twitter.com/kiraawc">twitter</a>

          </div>
        <div class="intro">
          Hi! I'm a PhD candidate in cognitive neuroscience and NIH NRSA fellow at <b>George Washington University</b>, advised by <a href=https://home.gwu.edu/~shom/ACL/publications.html>Sarah Shomstein</a>.
          <br><br>
          My research focuses on <b>semantic guidance of audiovisual attention</b>.
          I'm particularly interested in how real world multisensory scenes are percieved and attended to.
          <br><br>
          I am also interested in exploring <b>how data can be leveraged in health and technology</b> contexts.
          <br><br>
          Previously, I graduated from Rice University & worked as post bac reseacher at Baylor College of Medicine.
          <br><br>
        </div>

        </div>
      </div>

      <div class = "block3">

        <div class = publications>

          <a href="https://scholar.google.com/citations?user=tAiYT2IAAAAJ&hl=en&oi=ao">Full list also available on Google Scholar.</a>
          <br><br>
          * = equal contributions
          <br><br>
          <u>Work in preparation</u>
          <br><br><br>
          <b>Wegner-Clemens, K.</b>, Kravitz, D.J, Shomstein, S. Task irrelevance guidance of attention in audiovisual semantic relationships.
          <br><br><br>

          McEvoy, K.*, <b>Wegner-Clemens, K.*</b>, Auer, E., Eberhardt, S., Bernstein, L., Shomstein, S. Covert attention modulates visual speech perception independent of eye position.
          <br><br><br>

          Nag, S.*, Mahableshwarkar, P.*, <b>Wegner-Clemens, K.</b>, Cox, P., Kaplan, S., Teng, C., Kravitz, D., Mitroff, S. Efficiencies of online data collection.
          <br><br><br>

          Bean, S., <b>Wegner-Clemens, K.</b>, Shomstein, S., Malcolm, G.L. Semantically congruous sounds draw and hold gaze on objects across different scene backgrounds.
          <br><br><br>

          <b>Wegner-Clemens, K.</b>, Malcolm, G., Shomstein, Search efficiency scales with audiovisual semantic relatedness in a continuous manner.
          <br><br><br>
          <u>2024</u>
          <br><br><br>

          <b>Wegner-Clemens, K.</b>, Malcolm, G. L., Shomstein, S. (2024)
          Predicting attention in real-world environments: the need to investigate crossmodal semantic guidance.
          <i>WIRES Cognitive Science.</i>
          <a href="https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wcs.1675"> (link) </a>
          <br><br>
          <u>2022</u>
          <br><br><br>
        <b>Wegner-Clemens, K.</b>, Malcolm, G. L., Shomstein, S. (2022)
        How much is a cow like a meow? A novel database of human judgements of audiovisual semantic relatedness.
        <i>Attention, Perception, & Psychophysics.</i>
        (<a href="https://link.springer.com/article/10.3758/s13414-022-02488-1"> link; </a>
        <a href="https://psyarxiv.com/7h82c/"> preprint </a>)

        <br><br><br>
        <u>2020</u>
        <br><br><br>

        Magnotti, J.F., Dzeda, K.B., <b>Wegner-Clemens, K.</b>, Rennig, J., & Beauchamp, M.S. (2020).
        Weak observer-level correlation and strong stimulus-level correlation between the McGurk effect and
        audiovisual speech-in-noise: a causal inference explanation.</a> <i>Cortex.</i> doi:10.1016/j.cortex.2020.10.002
        (<a href = "pubs/Magnotti_et_al_Cortex_2020.pdf">pdf</a>; <a href="https://www.sciencedirect.com/science/article/pii/S0010945220303749">link</a>)

        <br><br><br>

        <strong>Wegner-Clemens, K.</strong>, Rennig, J., & Beauchamp, M.S. (2020) A relationship
        between Autism-Spectrum Quotient and face viewing behavior in 98 participants. <i>PLoS ONE</i> 15(4): e0230866.
        (<a href="pubs/Wegner-Clemens_et_al_2020_PLOSONE.pdf">pdf</a>;
        <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0230866">link</a>)


        <br><br><br>
        <u>2019</u>
        <br><br><br>

        <strong>Wegner-Clemens K</strong>, Rennig J, Magnotti JF, Beauchamp MS. (2019)
        Using principal components analysis to characterize eye movement fixation patterns during face viewing. <i>Journal of Vision</i>,
        November 2019, Vol.19, 2. doi:10.1167/19.13.2
        (<a href="pubs/Wegner-Clemens_Rennig_Magnotti_Beauchamp_JOV_2019.pdf">pdf</a>;
        <a href="https://jov.arvojournals.org/article.aspx?articleid=2755283">link</a>)

        <br><br><br>
        Rennig, J., <strong>Wegner-Clemens, K.</strong>, & Beauchamp, M.S. (2019)
        Face Viewing Behavior Predicts Multisensory Gain During Speech Perception. <i>Psychonomic Bulletin & Review</i>. 27, 70â€“77(2020)
        ( <a href="pubs/Rennig_WegnerClemens_Beauchamp_PBR2019.pdf">pdf</a>;
         <a href="https://link.springer.com/article/10.3758/s13423-019-01665-y">link</a>)

        <br><br><br>
        Convento, S., <strong>Wegner-Clemens, K. A.</strong>, & Yau, J. M. (2019).
        Reciprocal Interactions Between Audition and Touch in Flutter Frequency Perception</a>,
        <i>Multisensory Research</i>, 32(1), 67-85.
        (<a href="pubs/Convento_et_al_MultisensoryResarch.pdf">pdf</a>;
        <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7294791/">link</a>)

      </div>
    </div>

    <div class = "block4"
      <div class = projects>
        This page is still under construction!
        <br><br><br>
        <b> Semantic guidance of attention </b>
        <br><br>
        <i>Skills:</i>
        <br><br><br>
        <b> Understanding sound-image semantic relationships </b>
        <br><br>
        <i>Skills:</i>
        <br><br><br>
        <b> Audiovisual speech perception </b>
        <br><br>
        <i>Skills:</i>
        <br><br><br>
        <b> Novel analysis of eye tracking data in face viewing</b>
        <br><br>
        <i>Skills:</i>

      </div>
    </div>



  </body>

</html>
