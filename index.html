<!DOCTYPE html>
<html>

  <head>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
    <link rel="stylesheet" href="index.css">
    <script src="index.js"></script>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="icon" href="cat.svg" type="image/x-icon">

  </head>

  <body>

    <div class=header>
      <div class="name">Kira Wegner-Clemens, PhD</div>

      <div class="title">cognitive neuroscientist</div>

      <div class="navbar">
        <a href="#" id="homebutton" class="button">home</a>
        <a href="#" id="projbutton" class="button">research</a>
        <a href="#" id="pubbutton" class="button">publications</a>
        <a href="cv_kwegnerclemens.pdf">cv</a>
        <a href="https://scholar.google.com/citations?user=tAiYT2IAAAAJ&hl=en&oi=ao">google scholar</a>
      </div>

    </div>

    <div class="mainbox">
      <div class="sidebar">

        <div class="photo">
          <img src = "pic.jpg" width="150px"></img>
        </div>
      
      <div class="contact">
        kw1021
        @ georgetown edu 
      </div>
    </div>

    <div class="contentbox">

      <div class="intro">

    Hi! I'm a postdoctoral fellow at Georgetown University, working in Anna Greenwald's <a href="https://rhecolab.georgetown.edu/">Right Hemisphere Emotion and Cognition Lab</a> to study attention changes after right hemisphere stroke.
    <br><br>
    My primary research interest in attention in multisensory contexts. 
    <br><br>
    I completed my PhD at George Washington University in Sarah Shomstein's <a href="https://blogs.gwu.edu/shom/">Attention and Cognition Lab</a>, where my dissertation focused on how semantic understanding guides attention in audiovisual contexts.
    <br><br>
    Before grad school, I recieved my BA from Rice University and worked as a post bac researcher at Baylor College of Medicine. 
      </div>

      <div class="pub">
           <u>2024</u>
          <br><br>
          <b>Wegner-Clemens, K.</b>, Malcolm, G. L., Shomstein, S. (2024)
          Predicting attention in real-world environments: the need to investigate crossmodal semantic guidance.
          <i>WIRES Cognitive Science.</i>
          <a href="https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wcs.1675"> (link) </a>
          <br><br>
          <u>2022</u>
          <br><br>
        <b>Wegner-Clemens, K.</b>, Malcolm, G. L., Shomstein, S. (2022)
        How much is a cow like a meow? A novel database of human judgements of audiovisual semantic relatedness.
        <i>Attention, Perception, & Psychophysics.</i>
        (<a href="https://link.springer.com/article/10.3758/s13414-022-02488-1"> link; </a>
        <a href="https://psyarxiv.com/7h82c/"> preprint </a>)

        <br><br><br>
        <u>2020</u>
        <br><br>

        Magnotti, J.F., Dzeda, K.B., <b>Wegner-Clemens, K.</b>, Rennig, J., & Beauchamp, M.S. (2020).
        Weak observer-level correlation and strong stimulus-level correlation between the McGurk effect and
        audiovisual speech-in-noise: a causal inference explanation.</a> <i>Cortex.</i> doi:10.1016/j.cortex.2020.10.002
        (<a href = "pubs/Magnotti_et_al_Cortex_2020.pdf">pdf</a>; <a href="https://www.sciencedirect.com/science/article/pii/S0010945220303749">link</a>)

        <br><br><br>

        <strong>Wegner-Clemens, K.</strong>, Rennig, J., & Beauchamp, M.S. (2020) A relationship
        between Autism-Spectrum Quotient and face viewing behavior in 98 participants. <i>PLoS ONE</i> 15(4): e0230866.
        (<a href="pubs/Wegner-Clemens_et_al_2020_PLOSONE.pdf">pdf</a>;
        <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0230866">link</a>)


        <br><br><br>
        <u>2019</u>
        <br><br><br>

        <strong>Wegner-Clemens K</strong>, Rennig J, Magnotti JF, Beauchamp MS. (2019)
        Using principal components analysis to characterize eye movement fixation patterns during face viewing. <i>Journal of Vision</i>,
        November 2019, Vol.19, 2. doi:10.1167/19.13.2
        (<a href="pubs/Wegner-Clemens_Rennig_Magnotti_Beauchamp_JOV_2019.pdf">pdf</a>;
        <a href="https://jov.arvojournals.org/article.aspx?articleid=2755283">link</a>)

        <br><br><br>
        Rennig, J., <strong>Wegner-Clemens, K.</strong>, & Beauchamp, M.S. (2019)
        Face Viewing Behavior Predicts Multisensory Gain During Speech Perception. <i>Psychonomic Bulletin & Review</i>. 27, 70â€“77(2020)
        ( <a href="pubs/Rennig_WegnerClemens_Beauchamp_PBR2019.pdf">pdf</a>;
         <a href="https://link.springer.com/article/10.3758/s13423-019-01665-y">link</a>)

        <br><br><br>
        Convento, S., <strong>Wegner-Clemens, K. A.</strong>, & Yau, J. M. (2019).
        Reciprocal Interactions Between Audition and Touch in Flutter Frequency Perception</a>,
        <i>Multisensory Research</i>, 32(1), 67-85.
        (<a href="pubs/Convento_et_al_MultisensoryResarch.pdf">pdf</a>;
        <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7294791/">link</a>)   
      </div>

    <div class="proj">
      <b>Ongoing projects:</b> My work currently focus on understanding temporal attention and attention in complex dynamic scenes after stroke.
      <br><br><br>
      <b>Semantic guidance of audiovisual attention:</b> Real world environments are multimodal and semantically rich. 
      Prior work has shown semantic relatedness between visual objects guide what is attended in visual scenes in an automatic way, rather than a task strategy. 
      In my disseration work (adviced by Sarah Shomstein at George Washington Univeristy), we demonstrated that task irrelevant semantic guidance extends both across modality (e.g., between sounds and images) and in a graded manner (e.g., not simply between related, but in a scaled manner). 
      This project leveraged online data collection techniques to create a database of crossmodal semantic relatedness (<a href="https://link.springer.com/article/10.3758/s13414-022-02488-1">the Sight Sound Semantics database</a>) and investigate attentional questions.

      <br><br><br>
      <b>Spatial attention & speech perception:</b> 
      Visual information from the face and mouth can drastically improve speech perception, particularly in noisy environments. 
      However, how much visual information helps varies substantially between individuals. 
      During post bac resarch (advised by Micheal Beauchamp and Johannes Rennig at Baylor College of Medicine), we used eye tracking techniques and found that where individual look during easy to understand speech predicts how much they benefit from visual information during difficult to understand speech (<a href="https://link.springer.com/article/10.3758/s13423-019-01665-y">published here</a>). 
      <br><br>In graduate work (advised by Sarah Shomstein and Lynne Bernstein at George Washington University), we tested whether this effect could be explained by individual differences in covert attention, by holding gaze constant and manipulating covert attention using cues to the eyes or mouth (manuscript currently in preparation)
    </div>

    </div>

  </div>

  </body>
</html>
