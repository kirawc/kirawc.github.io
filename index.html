<!DOCTYPE html>
<html>

  <head>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
    <link rel="stylesheet" href="index.css">
    <script src="index.js"></script>

  </head>

  <body>

      <div class = "block1">
        <div class="name">Kira Wegner-Clemens</div>
        <div class="subhead">cognitive neuroscientist</div>

      <div class = "linksbox">
        <a href="#" id="homebutton" class="button">home</a>
        <a href="#" id="projbutton" class="button">projects</a>
        <a href="#" id="pubbutton" class="button">publications</a>
      </div>

      </div>

      <div class = "block2">
        <div class="flex-container">

          <div class='image'>
          <img src = "pic.jpg", width="150"></img>

          <br><br>
          kira at gwu dot edu
          <br><br>
          <a href="CV.pdf">CV</a>
          <br><br>
          <a href="https://scholar.google.com/citations?user=tAiYT2IAAAAJ&hl=en&oi=ao">google scholar</a> /
          <a href="https://github.com/kirawc">github</a>
          <br>
          <a href="https://www.linkedin.com/in/kira-wegner-clemens/">linkedin</a> /
          <a href="https://www.twitter.com/kiraawc">twitter</a>

          </div>
        <div class="intro">
          Hi! I'm a PhD candidate in cognitive neuroscience at George Washington University.
          <br><br>
          My research focuses on semantic guidance of audiovisual attention.
          I'm particularly interested in how real world multisensory scenes are percieved and attended to.
          <br><br>
          Before grad school, I studied cognitive science at Rice University & worked as post bac reseacher at Baylor College of Medicine.
          <br><br>
        </div>

        </div>
      </div>

      <div class = "block3">

        <div class = publications>

          <a href="https://scholar.google.com/citations?user=tAiYT2IAAAAJ&hl=en&oi=ao">Full list also available on Google Scholar.</a>
          <br><br>
          * = equal contributions
          <br><br>
          <u>Work in preparation</u>
          <br><br><br>
          <b>Wegner-Clemens, K.</b>, Kravitz, D.J, Shomstein, S. Task irrelevance guidance of attention in audiovisual semantic relationships.
          <br><br><br>

          McEvoy, K.*, <b>Wegner-Clemens, K.*</b>, Auer, E., Eberhardt, S., Bernstein, L., Shomstein, S. Covert attention modulates visual speech perception independent of eye position.
          <br><br><br>

          Nag, S.*, Mahableshwarkar, P.*, <b>Wegner-Clemens, K.</b>, Cox, P., Kaplan, S., Teng, C., Kravitz, D., Mitroff, S. Efficiencies of online data collection.
          <br><br><br>

          Bean, S., <b>Wegner-Clemens, K.</b>, Shomstein, S., Malcolm, G.L. Semantically congruous sounds draw and hold gaze on objects across different scene backgrounds.
          <br><br><br>

          <b>Wegner-Clemens, K.</b>, Malcolm, G., Shomstein, Search efficiency scales with audiovisual semantic relatedness in a continuous manner.
          <br><br><br>
          <u>2024</u>
          <br><br><br>

          <b>Wegner-Clemens, K.</b>, Malcolm, G. L., Shomstein, S. (2024)
          Predicting attention in real-world environments: the need to investigate crossmodal semantic guidance.
          <i>WIRES Cognitive Science.</i>
          <a href="https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wcs.1675"> (link) </a>
          <br><br>
          <u>2022</u>
          <br><br><br>
        <b>Wegner-Clemens, K.</b>, Malcolm, G. L., Shomstein, S. (2022)
        How much is a cow like a meow? A novel database of human judgements of audiovisual semantic relatedness.
        <i>Attention, Perception, & Psychophysics.</i>
        (<a href="https://link.springer.com/article/10.3758/s13414-022-02488-1"> link; </a>
        <a href="https://psyarxiv.com/7h82c/"> preprint </a>)

        <br><br><br>
        <u>2020</u>
        <br><br><br>

        Magnotti, J.F., Dzeda, K.B., <b>Wegner-Clemens, K.</b>, Rennig, J., & Beauchamp, M.S. (2020).
        Weak observer-level correlation and strong stimulus-level correlation between the McGurk effect and
        audiovisual speech-in-noise: a causal inference explanation.</a> <i>Cortex.</i> doi:10.1016/j.cortex.2020.10.002
        (<a href = "pubs/Magnotti_et_al_Cortex_2020.pdf">pdf</a>; <a href="https://www.sciencedirect.com/science/article/pii/S0010945220303749">link</a>)

        <br><br><br>

        <strong>Wegner-Clemens, K.</strong>, Rennig, J., & Beauchamp, M.S. (2020) A relationship
        between Autism-Spectrum Quotient and face viewing behavior in 98 participants. <i>PLoS ONE</i> 15(4): e0230866.
        (<a href="pubs/Wegner-Clemens_et_al_2020_PLOSONE.pdf">pdf</a>;
        <a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0230866">link</a>)


        <br><br><br>
        <u>2019</u>
        <br><br><br>

        <strong>Wegner-Clemens K</strong>, Rennig J, Magnotti JF, Beauchamp MS. (2019)
        Using principal components analysis to characterize eye movement fixation patterns during face viewing. <i>Journal of Vision</i>,
        November 2019, Vol.19, 2. doi:10.1167/19.13.2
        (<a href="pubs/Wegner-Clemens_Rennig_Magnotti_Beauchamp_JOV_2019.pdf">pdf</a>;
        <a href="https://jov.arvojournals.org/article.aspx?articleid=2755283">link</a>)

        <br><br><br>
        Rennig, J., <strong>Wegner-Clemens, K.</strong>, & Beauchamp, M.S. (2019)
        Face Viewing Behavior Predicts Multisensory Gain During Speech Perception. <i>Psychonomic Bulletin & Review</i>. 27, 70â€“77(2020)
        ( <a href="pubs/Rennig_WegnerClemens_Beauchamp_PBR2019.pdf">pdf</a>;
         <a href="https://link.springer.com/article/10.3758/s13423-019-01665-y">link</a>)

        <br><br><br>
        Convento, S., <strong>Wegner-Clemens, K. A.</strong>, & Yau, J. M. (2019).
        Reciprocal Interactions Between Audition and Touch in Flutter Frequency Perception</a>,
        <i>Multisensory Research</i>, 32(1), 67-85.
        (<a href="pubs/Convento_et_al_MultisensoryResarch.pdf">pdf</a>;
        <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7294791/">link</a>)

      </div>
    </div>

    <div class = "block4"
      <div class = projects>
        <i>This page is still under construction!</i>
        <br><br><br>
        <b>Semantic guidance of audiovisual attention:</b>
        <br>
        What we know about the world shapes how we attend to and percieve it.
        <br><br>
        My dissertation research investigated how meaningful sounds can modulate visual attentional priority,
        which is essential to understanding attention in complex real world scenes, as (<a href="https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/wcs.1675">reviewed here</a>!)
        <br><br>
        I used online data collection techniques (javascript, AWS servers, mTurk) to:
        <br><br>
        - create <a href="https://link.springer.com/article/10.3758/s13414-022-02488-1"> a freely available database</a> of
        semantic relatedness values based on human judgements
        <br><br>
        - demonstrate that a sound's semantic relationship to the target predicts visual search speeds (<a href="https://osf.io/preprints/psyarxiv/5cnam">preprint available</a>).
        <br><br>
        - investigate the importance of task relevance (ongoing work)
        <br><br>
        In ongoing work, I'm also using investigation the neural mechanisms investgating audiovisual semantic understandings and
        am learning modern MRI analysis techniques that leverage machine learning on complex biological data (MVPA, RSA).
        <br><br><br>
        <b>Eye movements & speech perception: </b>
        <br>
        Faces are extremely important and common visual objects, meaning they might be attended differently. In particularly noisy environments, visual information from the mouth can be crucial to understanding speech.
        <br><br>I have contributed to several projects investigating face and speech perception:
        <br><br>
        - developing a <a href="https://jov.arvojournals.org/article.aspx?articleid=2755283">novel eye tracking analysis for face viewing</a><br><br>
        - investigating the role of eye movements in speech perception (<a href="https://www.sciencedirect.com/science/article/pii/S0010945220303749">1</>,
          <a href="https://link.springer.com/article/10.3758/s13423-019-01665-y">2</a>)
          <br><br>
        - how face viewing behavior relates to social/behavioral traits (<a href="https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0230866">1</a>).
        <br><br>
        - the relationship between attention, face viewing preferenes, and speech perception (manuscript in prep)

        <br><br><br>

      </div>
    </div>



  </body>

</html>
